import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time

# 학생 1

# url 가져오기
driver = webdriver.Chrome(executable_path="./chromedriver.exe")
url = 'https://map.kakao.com/'
driver.get(url)
driver.maximize_window()

# 검색어 입력
time.sleep(2)
serch = driver.find_element_by_xpath(
    '//*[@id="dimmedLayer"]')
serch.click()
driver.find_element_by_xpath(
    '//*[@id="search.keyword.query"]').send_keys('강남 맛집')
serch = driver.find_element_by_xpath(
    '//*[@id="search.keyword.submit"]'
)
serch.click()
# 5페이지 정도 이름 크롤링 (이름,가격,배송비)
res = []
res.append('이름')
time.sleep(2)
for j in range(1, 18):
    try:
        name = driver.find_element_by_xpath(
            f'//*[@id="info.search.place.list"]/li[{j}]/div[3]/strong/a[2]').text
        print(f'---{name}---')
    except:
        name = "광고"
        pass
    res.append(name)
driver.execute_script("window.scrollTo(0, 100)") 
element = driver.find_element_by_id('info.search.place.more') 
driver.execute_script("arguments[0].click();", element)  


for k in range(3,7) :
    time.sleep(1)
    for h in range(1,17) :
        try:
            name2 = driver.find_element_by_xpath(                
            f'//*[@id="info.search.place.list"]/li[{h}]/div[3]/strong/a[2]').text
            print(f'---{name2}---')
        except:            
            name2 = "광고"
            pass
        res.append(name2)
        if k == 3 or k == 4 or k == 5 :
            serch = driver.find_element_by_xpath(
            f'//*[@id="info.search.page.no{k}"]').send_keys(Keys.ENTER)
              
print(res)
driver.quit()

#학생2

from bs4 import BeautifulSoup
from selenium import webdriver
from pprint import pprint

# webdriver 실행
dr = webdriver.Chrome(executable_path="./chromedriver.exe")
url = 'https://map.kakao.com/'
driver.get(url)
driver.maximize_window()

# target page 접근
dr.get("https://place.map.kakao.com/348276052")



soup = BeautifulSoup(html_source, 'html.parser')


products = soup.select('.product_list dd a')

print(products)
